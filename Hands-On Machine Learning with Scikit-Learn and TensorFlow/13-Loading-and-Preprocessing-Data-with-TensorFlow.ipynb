{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Loading and Preprocessing Data with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: (), types: tf.int32>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.range(10)  # any data tensor\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/hakan/OneDrive/Deep-Learning/deep-learning-notes/Hands-On Machine Learning with Scikit-Learn and TensorFlow/venv/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.repeat(3).batch(7)\n",
    ">>> for item in dataset:\n",
    "...     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: (?,), types: tf.int32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.map(lambda x: x * 2) # Items: [0,2,4,6,8,10,12]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n",
      "tf.Tensor([16 18], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    ">>> for item in dataset:\n",
    "...     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.apply(tf.data.experimental.unbatch()) # Items: 0,2,4,...\n",
    "# Each item in the new dataset will be a single integer tensor instead of a batch of 7 integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    ">>> for item in dataset:\n",
    "...     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x < 10) # Items: 0 2 4 6 8 0 2 4 6..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    ">>> for item in dataset:\n",
    "...     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(3):\n",
    "...     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([3 6], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(3) # 0 to 9, three times\n",
    "\n",
    ">>> dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\n",
    "\n",
    ">>> for item in dataset:\n",
    "...     print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTERLEAVING LINES FROM MULTIPLE FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filepaths = [ 'datasets/housing/train/housingaa.csv',\n",
    "                    'datasets/housing/train/housingab.csv',\n",
    "                    'datasets/housing/test/housingac.csv',\n",
    "                    'datasets/housing/test/housingad.csv',\n",
    "                    'datasets/housing/valid/housingae.csv',\n",
    "                    'datasets/housing/valid/housingaf.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_readers = 5\n",
    "\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'-117.56,33.94,6.0,575.0,73.0,318.0,88.0,7.0215,257100.0,INLAND'\n",
      "b'-118.13,33.86,37.0,2259.0,425.0,1183.0,413.0,5.1805,201600.0,<1H OCEAN'\n",
      "b'-118.62,34.17,34.0,3268.0,538.0,1463.0,519.0,6.8482,308300.0,<1H OCEAN'\n",
      "b'-122.47,37.75,52.0,1598.0,285.0,689.0,265.0,4.6071,337400.0,NEAR BAY'\n",
      "b'-119.31,36.06,20.0,2236.0,434.0,1405.0,412.0,1.8827,48700.0,INLAND'\n"
     ]
    }
   ],
   "source": [
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/housing/housing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7999, 10)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.concat([pd.read_csv('datasets/housing/train/housingaa.csv'), pd.read_csv('datasets/housing/train/housingab.csv')], axis=0)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and scale of each feature in the training set\n",
    "X_mean = df.loc[:, 'longitude':'median_income'].mean(axis=0).tolist()\n",
    "X_std = df.loc[:, 'longitude':'median_income'].std(axis=0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 8\n",
    "\n",
    "def preprocess(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32), tf.constant([], dtype=tf.string)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    \n",
    "    x = tf.stack(fields[:-2])\n",
    "    y = tf.stack(fields[-2:-1])\n",
    "    return (x - X_mean) / X_std, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting Everything Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, repeat=None, n_readers=5,\n",
    "                       n_read_threads=None, shuffle_buffer_size=10000,\n",
    "                       n_parse_threads=5, batch_size=32):\n",
    "    \n",
    "    dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)\n",
    "    \n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
    "    \n",
    "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Dataset With tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filepaths = [ 'datasets/housing/train/housingaa.csv',\n",
    "                    'datasets/housing/train/housingab.csv'\n",
    "]\n",
    "\n",
    "valid_filepaths = [ 'datasets/housing/valid/housingac.csv',\n",
    "                    'datasets/housing/valid/housingad.csv'\n",
    "]\n",
    "\n",
    "test_filepaths = [ 'datasets/housing/test/housingae.csv',\n",
    "                    'datasets/housing/test/housingaf.csv'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = csv_reader_dataset(train_filepaths, repeat=None)\n",
    "valid_set = csv_reader_dataset(valid_filepaths)\n",
    "test_set = csv_reader_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.30784070e+00  9.96340394e-01  1.85613656e+00 -2.80417532e-01\n",
      "  -3.48542362e-01 -3.49218458e-01 -3.91127497e-01 -7.39422560e-01]\n",
      " [-9.83412802e-01  1.81096601e+00  7.43750393e-01 -5.49025834e-01\n",
      "  -5.40765584e-01 -5.70859432e-01 -5.87293267e-01 -1.33789968e+00]\n",
      " [ 5.48881233e-01 -7.40589261e-01 -1.30267277e-01  8.09142113e-01\n",
      "   1.17500472e+00  2.64488578e-01  1.20958507e+00  1.60188273e-01]\n",
      " [-1.40766287e+00  1.10870326e+00  3.46469641e-01 -1.18800199e+00\n",
      "  -1.26219594e+00 -1.23843133e+00 -1.27779663e+00  1.33869916e-01]\n",
      " [-1.30784070e+00  1.01506782e+00  1.37939966e+00 -3.19837868e-01\n",
      "  -4.67198670e-01 -5.80572784e-01 -4.38207269e-01  2.11613953e-01]\n",
      " [-5.69143474e-01  1.26788282e+00 -1.00428498e+00 -6.31991923e-01\n",
      "  -7.28242576e-01 -7.65126407e-01 -7.49456942e-01  3.06570441e-01]\n",
      " [-1.38769770e+00  1.08997571e+00  4.25925791e-01 -5.53609610e-01\n",
      "  -4.81437415e-01 -1.35524780e-01 -4.72209334e-01 -6.97208107e-01]\n",
      " [-1.30784070e+00  1.01038682e+00  1.29994345e+00  3.76953818e-02\n",
      "  -2.06154779e-01 -2.82108039e-01 -1.19110987e-01  4.13527757e-01]\n",
      " [-1.35775185e+00  9.96340394e-01 -8.45372617e-01  1.08743653e-01\n",
      "   5.34260631e-01 -1.61132693e-01  4.27537560e-01  6.84026718e-01]\n",
      " [ 6.33730471e-01 -8.90405178e-01  9.02662694e-01 -2.46437509e-02\n",
      "   1.61679804e-01 -1.07267767e-01  2.31371820e-01 -1.58578545e-01]\n",
      " [ 6.53695703e-01 -7.54633904e-01  9.82118845e-01 -8.19467664e-01\n",
      "  -5.09914935e-01  2.22816761e-03 -4.87902582e-01 -1.21609879e+00]\n",
      " [ 5.53873479e-01 -7.63997614e-01  8.23206544e-01 -9.26727653e-01\n",
      "  -8.63510787e-01 -7.83670068e-01 -8.90696287e-01 -7.98217595e-01]\n",
      " [-1.12815773e+00  1.10402226e+00 -8.45372617e-01  4.82380278e-02\n",
      "  -2.10901022e-01  7.19876736e-02 -1.03417724e-01  4.66322094e-01]\n",
      " [-1.27789104e+00  9.49523568e-01 -7.65916467e-01 -8.13050449e-01\n",
      "  -8.80122662e-01 -8.99347186e-01 -8.27923238e-01  2.49986127e-01]\n",
      " [-1.37272477e+00  1.08529484e+00  5.05381942e-01 -4.69268441e-01\n",
      "  -4.36348021e-01 -4.75492001e-01 -4.40822810e-01 -6.80996001e-01]\n",
      " [ 5.63854158e-01 -7.54633904e-01  8.23206544e-01  2.94385910e-01\n",
      "   2.23381087e-01 -9.22562256e-02  3.07222575e-01  1.27434528e+00]\n",
      " [-1.34776735e+00  1.04783988e+00  1.08101182e-01 -5.36649704e-01\n",
      "  -2.34632283e-01 -4.97567773e-01 -1.94961742e-01 -1.10161448e+00]\n",
      " [-1.27290249e+00  9.58887279e-01  9.02662694e-01 -4.96312618e-01\n",
      "  -6.76033795e-01 -6.19426131e-01 -6.08217597e-01  2.68093079e-01]\n",
      " [-1.35275960e+00  1.05252266e+00  1.85613656e+00 -6.70953870e-01\n",
      "  -7.66212583e-01 -8.26938629e-01 -7.36379206e-01  2.42279842e-02]\n",
      " [-1.14313066e+00  1.10870326e+00 -1.00428498e+00  2.65966624e-01\n",
      "  -1.75304130e-01  6.93385750e-02 -1.21726528e-01  7.26030707e-01]\n",
      " [ 6.48707211e-01 -8.71677697e-01  8.23206544e-01 -4.14721698e-01\n",
      "  -1.25468478e-01  6.08870983e-01 -5.63379489e-02 -1.22841573e+00]\n",
      " [-1.35775185e+00  1.01975060e+00  1.37939966e+00  7.57405832e-02\n",
      "   2.51858592e-01  3.51025701e-01  2.91529328e-01 -1.52144325e+00]\n",
      " [-1.25293732e+00  1.02443159e+00 -1.95775878e+00 -2.70791650e-01\n",
      "  -7.14003801e-01 -5.25824785e-01 -6.47450745e-01  3.93269992e+00]\n",
      " [ 5.63854158e-01 -7.49952972e-01  9.82118845e-01 -6.74620867e-01\n",
      "  -6.47556245e-01 -7.63360322e-01 -6.50066257e-01  1.94980815e-01]\n",
      " [ 5.58865726e-01 -7.59316683e-01  6.64294243e-01 -5.99447191e-01\n",
      "  -6.83153152e-01 -7.05963254e-01 -7.10223794e-01  1.05058753e+00]\n",
      " [-1.21300697e+00  8.98024023e-01 -1.30267277e-01  1.58929801e+00\n",
      "   1.68760002e+00  1.41331279e+00  1.65945852e+00  1.51818991e-01]\n",
      " [-1.19803405e+00  1.09465849e+00 -3.68635714e-01 -5.98530471e-01\n",
      "  -8.42152655e-01 -6.79472327e-01 -7.70381272e-01  5.99282026e-01]\n",
      " [ 6.68668628e-01 -6.65681243e-01  5.84838092e-01  1.61456898e-01\n",
      "   4.29843068e-01 -6.04670830e-02  4.11844313e-01 -1.85949549e-01]\n",
      " [ 6.63680196e-01 -7.96769738e-01  2.67013490e-01 -8.51095617e-01\n",
      "  -7.99436331e-01 -4.98450816e-01 -7.33763635e-01 -1.47812343e+00]\n",
      " [ 6.38722718e-01 -7.63997614e-01  1.22048736e+00 -4.04637426e-01\n",
      "  -3.58034849e-01 -2.30009168e-01 -3.12661201e-01 -9.13386226e-01]\n",
      " [ 6.53695703e-01 -7.07817137e-01  1.29994345e+00  9.31588709e-02\n",
      "   3.80007416e-01  3.02458942e-01  4.01382118e-01 -3.89547616e-01]\n",
      " [ 6.48707211e-01 -7.92088807e-01  7.43750393e-01 -6.05864465e-01\n",
      "  -5.21780550e-01 -3.05066884e-01 -5.61137795e-01 -1.20193970e+00]] tf.Tensor(\n",
      "[[ 94700.]\n",
      " [ 39400.]\n",
      " [500001.]\n",
      " [212500.]\n",
      " [246900.]\n",
      " [144300.]\n",
      " [ 93900.]\n",
      " [231800.]\n",
      " [265400.]\n",
      " [276400.]\n",
      " [109600.]\n",
      " [235700.]\n",
      " [142500.]\n",
      " [163000.]\n",
      " [146100.]\n",
      " [465700.]\n",
      " [268800.]\n",
      " [196400.]\n",
      " [235600.]\n",
      " [161700.]\n",
      " [141300.]\n",
      " [129200.]\n",
      " [500001.]\n",
      " [350900.]\n",
      " [440100.]\n",
      " [332400.]\n",
      " [244100.]\n",
      " [268000.]\n",
      " [ 89800.]\n",
      " [105800.]\n",
      " [214000.]\n",
      " [ 89800.]], shape=(32, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_set.take(1):\n",
    "    print(x.numpy(), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    ...\n",
    "])\n",
    "\n",
    "model.compile([\n",
    "    ...\n",
    "])\n",
    "\n",
    "model.fit(train_set, steps_per_epoch=len(X_train) // batch_size, epochs=10,\n",
    "          validation_data=valid_set,\n",
    "          validation_steps=len(X_train) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_set, steps=len(X_test) // batch_size)\n",
    "model.predict(new_set, steps=len(X_new) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train(model, optimizer, loss_fn, n_epochs, [...]):\n",
    "    train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, [...])\n",
    "    for X_batch, y_batch in train_set:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Features API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_median_age = tf.feature_column.numeric_column(\"housing_median_age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_mean, age_std = X_mean[1], X_std[1]  # The median age is column in 1\n",
    "\n",
    "housing_median_age = tf.feature_column.numeric_column(\"housing_median_age\", normalizer_fn=lambda x: (x - age_mean) / age_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_income = tf.feature_column.numeric_column(\"median_income\")\n",
    "\n",
    "bucketized_income = tf.feature_column.bucketized_column(median_income, boundaries=[1.5, 3., 4.5, 6.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocean_prox_vocab = ['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN']\n",
    "\n",
    "ocean_proximity = tf.feature_column.categorical_column_with_vocabulary_list(\"ocean_proximity\", ocean_prox_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had a \"city\" feature in the dataset, we could encode it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_hash = tf.feature_column.categorical_column_with_hash_bucket(\"city\", hash_bucket_size=1000)\n",
    "# This feature will compute a hash for each category (i.e., for each city), modulo the number of hash buckets (hash_bucket_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to set the number of buckets high enough to avoid getting too many collisions (i.e., different categories ending up in the same bucket), but the higher we set it, the more RAM will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crossed Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketized_age = tf.feature_column.bucketized_column(housing_median_age, boundaries=[-1., -0.5, 0., 0.5, 1.]) # age was scaled\n",
    "\n",
    "age_and_ocean_proximity = tf.feature_column.crossed_column([bucketized_age, ocean_proximity], hash_bucket_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude = tf.feature_column.numeric_column(\"latitude\")\n",
    "longitude = tf.feature_column.numeric_column(\"longitude\")\n",
    "\n",
    "bucketized_latitude = tf.feature_column.bucketized_column(latitude, boundaries=list(np.linspace(32., 42., 20 - 1)))\n",
    "bucketized_longitude = tf.feature_column.bucketized_column(longitude, boundaries=list(np.linspace(-125., -114., 20 - 1)))\n",
    "\n",
    "location = tf.feature_column.crossed_column([bucketized_latitude, bucketized_longitude], hash_bucket_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding Categorical Features Using One-Hot Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two options to encode a categorical feature: one-hot vectors or embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocean_proximity_one_hot = tf.feature_column.indicator_column(ocean_proximity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding Categorical Features Using Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocean_proximity_embed = tf.feature_column.embedding_column(ocean_proximity,dimension=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Feature Columns for Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [bucketized_age, ....., median_house_value] # all features + target\n",
    "feature_descriptions = tf.feature_column.make_parse_example_spec(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_examples(serialized_examples):\n",
    "    examples = tf.io.parse_example(serialized_examples, feature_descriptions)\n",
    "    targets = examples.pop(\"median_house_value\") # separate the targets\n",
    "    return examples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = tf.data.TFRecordDataset([\"my_data_with_features.tfrecords\"])\n",
    "dataset = dataset.repeat().shuffle(10000).batch(batch_size).map(parse_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The TensorFlow Datasets (TFDS) Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "\u001b[1mDownloading and preparing dataset mnist (11.06 MiB) to /Users/hakan/tensorflow_datasets/mnist/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab0651b9487945caac47ddcd6cd68bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Dl Completed...', max=1, style=ProgressStyl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0255c8bed0745ce875661b0913616b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Dl Size...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cbf992693e44811873a902de46dd672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Extraction completed...', max=1, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a3fd36dd134b32b70b547162832aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2bdcf129e4460fa4f58bcaca19675a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Shuffling...', max=10, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0525 22:45:12.600831 4399105472 deprecation.py:323] From /Users/hakan/OneDrive/Deep-Learning/deep-learning-notes/Hands-On Machine Learning with Scikit-Learn and TensorFlow/venv/lib/python3.7/site-packages/tensorflow_datasets/core/file_format_adapter.py:247: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2085f11d3d42ddb27d272bd9dc2e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Reading...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e110382516bd4746be930acba55bd3e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Writing...', max=6000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda697678dc74fa29c6b21790afad8e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Reading...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34d62b8e901c43daa8d9b3f04f553f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Writing...', max=6000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e301ab5fdc4a04831b5a96cdc6454f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Reading...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8815a29f5a45829df1a956673372ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Writing...', max=6000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faca8558e94b40fe814c74d8270d9900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Reading...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c04d484b77745408e9422858a5cf8fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Writing...', max=6000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d47b405b9746ff9b6f4a82b830e4d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Reading...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14bec72dd8343bbae43589f40675284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Writing...', max=6000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dd8b4f4bc61450faea1317cef85b20f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Reading...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec28946b258846fca3fe015904e0b435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Writing...', max=6000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03dd522a38684cbdaac9104677a83cd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Reading...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d793d694a644a26982cf7f0131fad3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Writing...', max=6000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66eb80dc9784b3ab0b4e0c45968dc25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Reading...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37da7a8093e4e88b70031fa706571ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Writing...', max=6000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c31f391bd636492cb94164bd7fc3c649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Reading...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6fb0a02343741f48188f59533dacc4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Writing...', max=6000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886290d6b67e418a941a39c2c7a8d01c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Reading...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "461926ad4854441594785e221e4006c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Writing...', max=6000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ec36b506714fdda65b5968b1b10596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "901cb4eae9e34e8db9a922b65a80b06d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Shuffling...', max=1, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb20c74c31e4a169085b29da5637fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Reading...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af84ede3d85491091d5bcce4452c4f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Writing...', max=10000, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0525 22:45:24.835080 4399105472 deprecation.py:323] From /Users/hakan/OneDrive/Deep-Learning/deep-learning-notes/Hands-On Machine Learning with Scikit-Learn and TensorFlow/venv/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to /Users/hakan/tensorflow_datasets/mnist/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset = tfds.load(name=\"mnist\")\n",
    "mnist_train, mnist_test = dataset[\"train\"], dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets\n",
    "!ip install widgetsnbextension\n",
    "!pip install tqdm\n",
    "!jupyter nbextension enable --py --sys-prefix widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = mnist_train.repeat(5).batch(32).prefetch(1)\n",
    "for item in mnist_train:\n",
    "    images = item[\"image\"]\n",
    "    labels = item[\"label\"]\n",
    "    [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = mnist_train.repeat(5).batch(32)\n",
    "mnist_train = mnist_train.map(lambda items: (items[\"image\"], items[\"label\"]))\n",
    "mnist_train = mnist_train.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tfds.load(name=\"mnist\", batch_size=32, as_supervised=True)\n",
    "\n",
    "mnist_train = dataset[\"train\"].repeat().prefetch(1)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    \n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\")\n",
    "model.fit(mnist_train, steps_per_epoch=60000 // 32, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
